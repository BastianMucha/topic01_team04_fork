{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Notizen & offene Fragen sind immer in rot eingef√ºgt\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation and comparison of K-nearest neighbors (KNN) and convolutional neural network (CNN) for clothes recognition\n",
    "### Data Analysis MoBi SoSe2023, Topic 01: Image Analysis\n",
    "### Tutor: Hannah Winter\n",
    "### Team 04: Ole Decker, Heinrike Gilles, Bastian Mucha, Anastasia Warken\n",
    "#### July 2023\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "<span style=\"color:red\">\n",
    "- What's KNN, what's CNN <br>\n",
    "- Whats the goal of our project? <br>\n",
    "- What is our extra work (zB KD Trees, special CNN for shirts and T-Shirts) <br>\n",
    "- Main result: KNN worked suprisingly good compared to CNN <br>\n",
    "</span>\n",
    "\n",
    "K-nearest neighbors is a classical machine learning algorithm that was first introduced in \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "### Z-transformation\n",
    "A Z-transformation is a statistical technique used to standardize and normalize data. It transforms a dataset into a standard normal distribution, which has a mean of 0 and a standard deviation of 1. It is performed by subtracting the mean of the dataset from each datapoint and then dividing it by the standard deviation. \n",
    "\n",
    "<span style=\"color:red\">\n",
    "here insertion of the Z-transformation formula??\n",
    "</span>\n",
    "\n",
    "Regarding the MNIST fashion dataset the Z-Transformation is applied to the intensity value of each pixel of each image. This is done to improve the performance and convergence of our KNN and CNN. Standardizing the pixel intensities helps to avoid that certain features dominate the learning process, e.g. high intensity pixels in the background of the images. Normalizing the data makes the pixels comparable to one another. \n",
    "\n",
    "### PCA\n",
    "A Principal Component Analysis (PCA) is a method used to reduce the dimensions of a high dimensional dataset while preserving the most important information and minimizing the loss of variance. This technique is done as a preparation to make a machine learnig process more efficient by reducing redundancy.\n",
    "To perform a PCA a Covariance matrix is calculated using the Z-transformed dataset. The covariance matrix gives the relationship of each pixel intensity value of all images. The columns of the covariance matrix are the eigenvectors. The eigenvalues are calculated using the covariance matrix. Each eigenvalue corresponds to an eigenvector and represents the explained variance of this vector. The larger the eigenvalue, the more variance is captured along that eigenvector or principal component. \n",
    "\n",
    "\n",
    "<span style=\"color:red\">\n",
    "in our code: we removed the algebraic sign of eigenvalues and then we ordered the eigenvalues in descending order again, bc negative eigenvalues mean that the corresponding eigenvector explains a negative variance, which gives more information than a eigenvector that explains close to 0 variance <br>\n",
    "from notes: <br>\n",
    "- Computation of eigenvectors and eigenvalues <br>\n",
    "    -> Graphically: Diagonalization translates to a rotation of correlation matrix, base vectors being PCs\n",
    "</span>\n",
    "\n",
    "- Discarding redundant PCs \n",
    "- coded with numpy (from scratch), short explanation of most important implementation steps"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
