{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first chunk loads and formats all the relevant data. To switch between the fashion and the digit dataset, adjust the \"dataset\" variable. To summarize the shirts and t-shirts for the two-step neural network, adjust the \"summarize_shirt\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pnd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#load data\n",
    "dataset = 1\n",
    "match dataset:\n",
    "    case 1:\n",
    "        test = 'fashion-mnist_test.csv'\n",
    "        train = 'fashion-mnist_train.csv'\n",
    "    case 2: \n",
    "        test = \"mnist_test.csv\"\n",
    "        train = \"mnist_train.csv\"\n",
    "testdata = pnd.read_csv(test)\n",
    "traindata = pnd.read_csv(train)\n",
    "\n",
    "#split into images and labels\n",
    "testdata_pixel = testdata.drop(testdata.columns[0], axis=1).to_numpy()\n",
    "traindata_pixel = traindata.drop(traindata.columns[0], axis=1).to_numpy()\n",
    "label_train = traindata[traindata.columns[0]].to_numpy()\n",
    "label_test = testdata[testdata.columns[0]].to_numpy()\n",
    "og_label_train = label_train.copy()\n",
    "og_label_test = label_test.copy()\n",
    "\n",
    "#reshape for cnn\n",
    "cnn_test = testdata_pixel.reshape(-1,28,28) /255.0\n",
    "cnn_train = traindata_pixel.reshape(-1,28,28) /255.0\n",
    "cnn_test = np.expand_dims(cnn_test,axis=3)\n",
    "cnn_train = np.expand_dims(cnn_train,axis=3)\n",
    "cnn_label_test = tf.keras.utils.to_categorical(label_test)\n",
    "cnn_label_train = tf.keras.utils.to_categorical(label_train)\n",
    "\n",
    "softmax_nodes = 10\n",
    "class_names = np.array([\"T-shirt / Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"])\n",
    "\n",
    "#summarize the classes 0 and 6\n",
    "summarize_shirt = False\n",
    "if summarize_shirt == True:\n",
    "    label_test[label_test==6]=0\n",
    "    label_test[label_test>=7]-=1\n",
    "    label_train[label_train==6]=0\n",
    "    label_train[label_train>=7]-=1\n",
    "    cnn_label_test = tf.keras.utils.to_categorical(label_test)\n",
    "    cnn_label_train = tf.keras.utils.to_categorical(label_train)\n",
    "    softmax_nodes -= 1\n",
    "    class_names = np.delete(class_names,6)\n",
    "\n",
    "    shirts_pixel = traindata_pixel[(og_label_train==6)|(og_label_train==0)]\n",
    "    shirts_pixel = shirts_pixel.reshape(-1,28,28) /255.0\n",
    "    shirts_pixel = np.expand_dims(shirts_pixel,axis=3)\n",
    "    label_shirts = og_label_train[(og_label_train==6)|(og_label_train==0)]\n",
    "    label_shirts[label_shirts==6]=1\n",
    "    label_shirts = tf.keras.utils.to_categorical(label_shirts)\n",
    "\n",
    "    shirts_pixel_val = testdata_pixel[(og_label_test==6)|(og_label_test==0)]\n",
    "    shirts_pixel_val = shirts_pixel_val.reshape(-1,28,28) /255.0\n",
    "    shirts_pixel_val = np.expand_dims(shirts_pixel_val,axis=3)\n",
    "    label_shirts_val = og_label_test[(og_label_test==6)|(og_label_test==0)].copy()\n",
    "    label_shirts_val[label_shirts_val==6]=1\n",
    "    label_shirts_val = tf.keras.utils.to_categorical(label_shirts_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk allows to load custum images by adjusting the filepath of the variable \"your_image\". The image is then converted to grayscale and classified using our most complex cnn model. The current default is an image of the beloved character \"Bernd das Brot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 447ms/step\n",
      "T-shirt / Top: 0.089\n",
      "Trouser: 0.0\n",
      "Pullover: 0.007\n",
      "Dress: 0.0\n",
      "Coat: 0.0\n",
      "Sandal: 0.0\n",
      "Shirt: 0.896\n",
      "Sneaker: 0.0\n",
      "Bag: 0.007\n",
      "Ankle Boot: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def convert_image_to_grayscale(image_path):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "    # Resize the image to 28x28 pixels\n",
    "    resized_image = image.resize((28, 28))\n",
    "\n",
    "    # Convert the image to a NumPy array\n",
    "    numpy_image = np.array(resized_image)\n",
    "\n",
    "    # Normalize intensity values between 0 and 255\n",
    "    normalized_image = (numpy_image / np.max(numpy_image)) * -255\n",
    "\n",
    "    # Convert intensity values to integers\n",
    "    final_image = normalized_image.astype(np.uint8)\n",
    "\n",
    "    return final_image\n",
    "your_image = \"Report/Plots/bernd.jpg\"\n",
    "your_image = convert_image_to_grayscale(your_image)\n",
    "plt.imshow(your_image,cmap=\"gray\")\n",
    "#plt.savefig('Report/Plots/bernd.svg',format=\"svg\")\n",
    "your_image = your_image.reshape(-1,28,28) / 255.0\n",
    "your_image = np.expand_dims(your_image,axis=3)\n",
    "model = tf.keras.models.load_model('Results/model_shirts_notsum')\n",
    "for i,prob in enumerate(np.round(model.predict(your_image)[0],3)):\n",
    "    print(str(class_names[i])+\": \" + str(prob))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded and reshaped the data. The labels were converted into 10-dimensional to be compatible with the output of the softmax activation function and the dimension of the images was increased by one to show that there is only one color channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet5_model = Sequential([\n",
    "  Conv2D(6, 5, input_shape=(28, 28, 1), activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  Conv2D(16, 5, activation=\"relu\",padding=\"valid\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  Conv2D(120, 5, activation=\"relu\",padding=\"valid\"),\n",
    "  Flatten(),\n",
    "  Dense(84,activation=\"relu\"),\n",
    "  Dense(softmax_nodes, activation='softmax'),\n",
    "])\n",
    "lenet5_model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "lenet5_model.fit(\n",
    "  cnn_train,\n",
    "  cnn_label_train,\n",
    "  epochs=20,\n",
    "  validation_data=(cnn_test, cnn_label_test)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code used a modifyed LeNet-5 architecture with max pooling instead of mean pooling and ReLU activation. The next two chunks allow to train two slightly different modified versions of the VGG architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = Sequential([\n",
    "  Conv2D(32, 3, input_shape=(28, 28, 1), activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(32, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=1),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=1),\n",
    "  BatchNormalization(),\n",
    "  Flatten(),\n",
    "  Dense(256,activation=\"relu\"),\n",
    "  Dropout(0.15),\n",
    "  Dense(256,activation=\"relu\"),\n",
    "  Dropout(0.15),\n",
    "  Dense(softmax_nodes, activation='softmax'),\n",
    "])\n",
    "vgg_model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "vgg_model.fit(\n",
    "  cnn_train,\n",
    "  cnn_label_train,\n",
    "  epochs=20,\n",
    "  validation_data=(cnn_test, cnn_label_test),\n",
    "  batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_model = Sequential([\n",
    "  Conv2D(16, 5, input_shape=(28, 28, 1), activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 5, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(32, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(32, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=1),\n",
    "  BatchNormalization(),\n",
    "  Flatten(),\n",
    "  Dense(256,activation=\"relu\"),\n",
    "  Dropout(0.1),\n",
    "  Dense(512,activation=\"relu\"),\n",
    "  Dropout(0.1),\n",
    "  Dense(softmax_nodes, activation='softmax'),\n",
    "])\n",
    "complex_model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0.01,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=0\n",
    ")\n",
    "history = complex_model.fit(\n",
    "  cnn_train,\n",
    "  cnn_label_train,\n",
    "  epochs = 5,\n",
    "  validation_data=(cnn_test, cnn_label_test),\n",
    "  batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk allows to load a model of your choice and create the modifyed confusion matrix as described in the report and show the architecture using the visualkeras package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('Results/complex_model')\n",
    "y_pred = model.predict(cnn_test)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "conf = sklearn.metrics.confusion_matrix(y_pred, label_test)\n",
    "conf_df = pnd.DataFrame(conf, index=class_names, columns=class_names)\n",
    "rowsums = conf_df.sum(axis=1)\n",
    "acc = np.sum(y_pred == label_test)/ len(label_test)\n",
    "for i in range(len(conf_df)):\n",
    "    for j in range(len(conf_df.columns)):\n",
    "        if i != j:\n",
    "            conf_df.values[i,j] -= (1-acc)*rowsums[i]/(len(conf_df)-1)\n",
    "        else: \n",
    "            conf_df.values[i,j] -= acc*rowsums[i]\n",
    "sns.heatmap(conf_df, annot=True, cmap= \"RdBu\", annot_kws={\"fontsize\":6},fmt=\",.1f\",center=0)\n",
    "import visualkeras \n",
    "visualkeras.layered_view(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two chunks train a model that only distinguishes between shirts and t-shirts and to then make predictions making the two-stage model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet5_shirt_model = Sequential([\n",
    "  Conv2D(6, 5, input_shape=(28, 28, 1), activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  Conv2D(16, 5, activation=\"relu\",padding=\"valid\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  Conv2D(120, 5, activation=\"relu\",padding=\"valid\"),\n",
    "  Flatten(),\n",
    "  Dense(84,activation=\"relu\"),\n",
    "  Dense(2, activation='softmax'),\n",
    "])\n",
    "lenet5_shirt_model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "lenet5_shirt_model.fit(\n",
    "  shirts_pixel,\n",
    "  label_shirts,\n",
    "  epochs=75,\n",
    "  validation_data = (shirts_pixel_val,label_shirts_val),\n",
    "  batch_size = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shirts_sum = model.load(\"Results/model_shirts_sum/\")\n",
    "predictions = model_shirts_sum.predict(cnn_test)\n",
    "predictions =np.argmax(predictions, axis=1)\n",
    "predictions[predictions>=6]+=1\n",
    "prediction_shirt = cnn_test[predictions==0]\n",
    "prediction_shirt_final = lenet5_shirt_model.predict(prediction_shirt)\n",
    "prediction_shirt_final = np.argmax(prediction_shirt_final,axis=1) \n",
    "index = np.where(predictions==0)[0]\n",
    "for i in range(np.sum(predictions==0)):\n",
    "    if prediction_shirt_final[i]==1:\n",
    "        predictions[index[i]] = 6\n",
    "print(np.sum(predictions==og_label_test)/len(og_label_test))\n",
    "print(np.sum(predictions==6))\n",
    "y_pred = predictions\n",
    "y_true = og_label_test \n",
    "class_names = np.array([\"T-shirt / Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"])\n",
    "conf = confusion_matrix(y_pred, y_true)\n",
    "conf_df = pnd.DataFrame(conf, index=class_names, columns=class_names)\n",
    "print(conf_df)\n",
    "sns.heatmap(conf_df, annot=True, cmap='RdYlGn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compmeth2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
