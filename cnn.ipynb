{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pnd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#load data\n",
    "dataset = 1\n",
    "match dataset:\n",
    "    case 1:\n",
    "        test = 'fashion-mnist_test.csv'\n",
    "        train = 'fashion-mnist_train.csv'\n",
    "    case 2: \n",
    "        test = \"mnist_test.csv\"\n",
    "        train = \"mnist_train.csv\"\n",
    "testdata = pnd.read_csv(test)\n",
    "traindata = pnd.read_csv(train)\n",
    "\n",
    "#split into images and labels\n",
    "testdata_pixel = testdata.drop(testdata.columns[0], axis=1).to_numpy()\n",
    "traindata_pixel = traindata.drop(traindata.columns[0], axis=1).to_numpy()\n",
    "label_train = traindata[traindata.columns[0]].to_numpy()\n",
    "label_test = testdata[testdata.columns[0]].to_numpy()\n",
    "og_label_train = label_train.copy()\n",
    "og_label_test = label_test.copy()\n",
    "\n",
    "#reshape for cnn\n",
    "cnn_test = testdata_pixel.reshape(-1,28,28) /255.0\n",
    "cnn_train = traindata_pixel.reshape(-1,28,28) /255.0\n",
    "cnn_test = np.expand_dims(cnn_test,axis=3)\n",
    "cnn_train = np.expand_dims(cnn_train,axis=3)\n",
    "cnn_label_test = tf.keras.utils.to_categorical(label_test)\n",
    "cnn_label_train = tf.keras.utils.to_categorical(label_train)\n",
    "\n",
    "softmax_nodes = 10\n",
    "class_names = np.array([\"T-shirt / Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"])\n",
    "\n",
    "#summarize the classes 0 and 6\n",
    "summarize_shirt = False\n",
    "if summarize_shirt == True:\n",
    "    label_test[label_test==6]=0\n",
    "    label_test[label_test>=7]-=1\n",
    "    label_train[label_train==6]=0\n",
    "    label_train[label_train>=7]-=1\n",
    "    cnn_label_test = tf.keras.utils.to_categorical(label_test)\n",
    "    cnn_label_train = tf.keras.utils.to_categorical(label_train)\n",
    "    softmax_nodes -= 1\n",
    "    class_names = np.delete(class_names,6)\n",
    "\n",
    "    shirts_pixel = traindata_pixel[(og_label_train==6)|(og_label_train==0)]\n",
    "    shirts_pixel = shirts_pixel.reshape(-1,28,28) /255.0\n",
    "    shirts_pixel = np.expand_dims(shirts_pixel,axis=3)\n",
    "    label_shirts = og_label_train[(og_label_train==6)|(og_label_train==0)]\n",
    "    label_shirts[label_shirts==6]=1\n",
    "    label_shirts = tf.keras.utils.to_categorical(label_shirts)\n",
    "\n",
    "    shirts_pixel_val = testdata_pixel[(og_label_test==6)|(og_label_test==0)]\n",
    "    shirts_pixel_val = shirts_pixel_val.reshape(-1,28,28) /255.0\n",
    "    shirts_pixel_val = np.expand_dims(shirts_pixel_val,axis=3)\n",
    "    label_shirts_val = og_label_test[(og_label_test==6)|(og_label_test==0)].copy()\n",
    "    label_shirts_val[label_shirts_val==6]=1\n",
    "    label_shirts_val = tf.keras.utils.to_categorical(label_shirts_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def convert_image_to_grayscale(image_path):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "    # Resize the image to 28x28 pixels\n",
    "    resized_image = image.resize((28, 28))\n",
    "\n",
    "    # Convert the image to a NumPy array\n",
    "    numpy_image = np.array(resized_image)\n",
    "\n",
    "    # Normalize intensity values between 0 and 255\n",
    "    normalized_image = (numpy_image / np.max(numpy_image)) * -255\n",
    "\n",
    "    # Convert intensity values to integers\n",
    "    final_image = normalized_image.astype(np.uint8)\n",
    "\n",
    "    return final_image\n",
    "your_image = \"Plots/bernd.jpg\"\n",
    "your_image = convert_image_to_grayscale(your_image)\n",
    "plt.imshow(your_image,cmap=\"gray\")\n",
    "#plt.savefig('Plots/bernd.svg',format=\"svg\")\n",
    "your_image = your_image.reshape(-1,28,28) / 255.0\n",
    "your_image = np.expand_dims(your_image,axis=3)\n",
    "model = tf.keras.models.load_model('Results/model_shirts_notsum')\n",
    "for i,prob in enumerate(np.round(model.predict(your_image)[0],3)):\n",
    "    print(str(class_names[i])+\": \" + str(prob))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded and reshaped the data. The labels were converted into 10-dimensional to be compatible with the output of the softmax activation function and the dimension of the images was increased by one to show that there is only one color channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet5_model = Sequential([\n",
    "  Conv2D(6, 5, input_shape=(28, 28, 1), activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  Conv2D(16, 5, activation=\"relu\",padding=\"valid\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  Conv2D(120, 5, activation=\"relu\",padding=\"valid\"),\n",
    "  Flatten(),\n",
    "  Dense(84,activation=\"relu\"),\n",
    "  Dense(softmax_nodes, activation='softmax'),\n",
    "])\n",
    "lenet5_model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "lenet5_model.fit(\n",
    "  cnn_train,\n",
    "  cnn_label_train,\n",
    "  epochs=20,\n",
    "  validation_data=(cnn_test, cnn_label_test)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code used a modifyed LeNet-5 architecture with max pooling instead of mean pooling and ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = Sequential([\n",
    "  Conv2D(32, 3, input_shape=(28, 28, 1), activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(32, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=1),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=1),\n",
    "  BatchNormalization(),\n",
    "  Flatten(),\n",
    "  Dense(256,activation=\"relu\"),\n",
    "  Dropout(0.15),\n",
    "  Dense(256,activation=\"relu\"),\n",
    "  Dropout(0.15),\n",
    "  Dense(softmax_nodes, activation='softmax'),\n",
    "])\n",
    "vgg_model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "vgg_model.fit(\n",
    "  cnn_train,\n",
    "  cnn_label_train,\n",
    "  epochs=20,\n",
    "  validation_data=(cnn_test, cnn_label_test),\n",
    "  batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_model = Sequential([\n",
    "  Conv2D(16, 5, input_shape=(28, 28, 1), activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(16, 5, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(32, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(32, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  BatchNormalization(),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  Conv2D(64, 3, activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=1),\n",
    "  BatchNormalization(),\n",
    "  Flatten(),\n",
    "  Dense(256,activation=\"relu\"),\n",
    "  Dropout(0.1),\n",
    "  Dense(512,activation=\"relu\"),\n",
    "  Dropout(0.1),\n",
    "  Dense(softmax_nodes, activation='softmax'),\n",
    "])\n",
    "complex_model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0.01,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=0\n",
    ")\n",
    "history = complex_model.fit(\n",
    "  cnn_train,\n",
    "  cnn_label_train,\n",
    "  epochs = 5,\n",
    "  validation_data=(cnn_test, cnn_label_test),\n",
    "  batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('Results/complex_model')\n",
    "y_pred = model.predict(cnn_test)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "conf = sklearn.metrics.confusion_matrix(y_pred, label_test)\n",
    "conf_df = pnd.DataFrame(conf, index=class_names, columns=class_names)\n",
    "rowsums = conf_df.sum(axis=1)\n",
    "acc = np.sum(y_pred == label_test)/ len(label_test)\n",
    "for i in range(len(conf_df)):\n",
    "    for j in range(len(conf_df.columns)):\n",
    "        if i != j:\n",
    "            conf_df.values[i,j] -= (1-acc)*rowsums[i]/(len(conf_df)-1)\n",
    "        else: \n",
    "            conf_df.values[i,j] -= acc*rowsums[i]\n",
    "sns.heatmap(conf_df, annot=True, cmap= \"RdBu\", annot_kws={\"fontsize\":6},fmt=\",.1f\",center=0)\n",
    "# import visualkeras \n",
    "# visualkeras.layered_view(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet5_shirt_model = Sequential([\n",
    "  Conv2D(6, 5, input_shape=(28, 28, 1), activation=\"relu\",padding=\"same\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  Conv2D(16, 5, activation=\"relu\",padding=\"valid\"),\n",
    "  MaxPooling2D(pool_size=2,strides=2),\n",
    "  Conv2D(120, 5, activation=\"relu\",padding=\"valid\"),\n",
    "  Flatten(),\n",
    "  Dense(84,activation=\"relu\"),\n",
    "  Dense(2, activation='softmax'),\n",
    "])\n",
    "lenet5_shirt_model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "lenet5_shirt_model.fit(\n",
    "  shirts_pixel,\n",
    "  label_shirts,\n",
    "  epochs=75,\n",
    "  validation_data = (shirts_pixel_val,label_shirts_val),\n",
    "  batch_size = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = complex_model.predict(cnn_test)\n",
    "predictions =np.argmax(predictions, axis=1)\n",
    "predictions[predictions>=6]+=1\n",
    "prediction_shirt = cnn_test[predictions==0]\n",
    "prediction_shirt_final = lenet5_shirt_model.predict(prediction_shirt)\n",
    "prediction_shirt_final = np.argmax(prediction_shirt_final,axis=1) \n",
    "index = np.where(predictions==0)[0]\n",
    "for i in range(np.sum(predictions==0)):\n",
    "    if prediction_shirt_final[i]==1:\n",
    "        predictions[index[i]] = 6\n",
    "print(np.sum(predictions==og_label_test)/len(og_label_test))\n",
    "print(np.sum(predictions==6))\n",
    "y_pred = predictions\n",
    "y_true = og_label_test #label seems to be wrong\n",
    "class_names = np.array([\"T-shirt / Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"])\n",
    "conf = confusion_matrix(y_pred, y_true)\n",
    "conf_df = pnd.DataFrame(conf, index=class_names, columns=class_names)\n",
    "print(conf_df)\n",
    "sns.heatmap(conf_df, annot=True, cmap='RdYlGn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compmeth2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
