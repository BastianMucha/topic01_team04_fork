{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation and comparison of K-nearest neighbors (KNN) and convolutional neural network (CNN) for clothes recognition\n",
    "### Data Analysis MoBi SoSe2023, Topic 01: Image Analysis\n",
    "#### Tutor: Hannah Winter\n",
    "#### Team 04: Ole Decker, Heinrike Gilles, Bastian Mucha, Anastasia Warken\n",
    "##### July 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "In this work, the image classification algorithms K-nearest neighbors (KNN) and Convolutional Neural Network (CNN) were investigated and compared by testing both on the Fashion-MNIST dataset, featuring images of different clothing items of 10 categories. The preparation of the dataset included performing a Z-transformation and data dimension reduction using Principal Component Analysis (PCA). The optimal amount of variance and number of k for KNN was determined. To improve KNN efficiency, KD-Trees were implemented. The performance of the two algorithms were compared and evaluated. The CNN yielded a visibly better accuracy compared to the KNN, although the KNNs accuracy was still higher than expected. These results met the expectations, since KNN is a basic algorithm, while CNN is a modern, state-of-the-art deep learning model. \n",
    "In a modified confusion matrix it was found, that in both algorithms the categories \"T-Shirt/ Top\" and \"Shirt\" were often wrongly classified. In an attempt to overcome this confusion a CNN was implemented grouping these two classes together followed by a seperate CNN only classifying between the categories \"T-Shirt/ Top\" and \"Shirt\". \n",
    "\n",
    "<span style='color:lightblue'>\n",
    "clearly\n",
    "- Ergänzen: War T-Shirt / Top erfolgreich?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material\n",
    "\n",
    "### Imports\n",
    "\n",
    "For coding, optimization and visualization several functions from the packages numpy, pandas, seaborn, matplotlib and tensorflow keras were used.\n",
    "An overview over all imports can be found in the imports.yml file in the Report folder in our repository. <br>\n",
    "#### kann man weglassen!!The self coded functions for the Z-transformation and PCA are found in the PCA.py file in the functions folder. Our KNN function is stored in the identically named file. Since our CNN is not self coded, there is no CNN function but rather an CNN.ipynb file.\n",
    "<span style='color:lightblue'>\n",
    "mMn wäre es relevant zu erwähnen, dass tensorflow keras benutzt wurde. So wie ich das Verstanden habe ist tensorflow nur das Grundgerüst, die ganzen Machine Learning Sachen kommen durch Keras Tensor\n",
    "</span>\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset applied is Zalandos Fashion-MNIST dataset. MNIST stands for Modified National Institute of Standards and Technology database. The dataset consists of 70,000 images of Zalando clothing articles and is often used for benchmarking machine learning algorithms. Each image has the size of 28 x 28 pixels in grayscale and has been categorized into one of 10 classes, which include T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag and Ankle boot. The dataset is split into a training set containing 60,000 images and a test set comprising 10,000 images. The dataset is in form of a CSV file (comma separated value). Each row contains the information for one image. The first column of values consists of the class of each image ranging from 0 to 9, the following columns are the intensity values of each pixel, varying between 0 and 255.\n",
    "These are 100 example images of the clothing items. \n",
    "<span style='color:red'>\n",
    "All items fill the image to a comparable extend and the item is located in the center of the image. <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "### Z-transformation\n",
    "A Z-transformation is a statistical technique used to standardize and normalize data. It transforms a dataset into a standard normal distribution with a mean of 0 and a standard deviation of 1. It is performed by subtracting the mean of the dataset from each datapoint and then dividing it by the standard deviation. \n",
    "The Z-Transformation is applied to the intensity value of each pixel of each image, in order to improve performance and convergence of our KNN and CNN. Standardizing the pixel intensities helps to avoid certain features dominating the learning process, e.g. high intensity pixels in the background of the images. Normalizing the data improves pixel comparability\n",
    "\n",
    "### PCA\n",
    "A Principal Component Analysis (PCA) is a method used to reduce the dimensions of a high dimensional dataset while preserving the most important information and minimizing the loss of variance. This technique is done as a preparation to make a machine learnig process more efficient by reducing redundancy.\n",
    "To perform a PCA a Covariance matrix is calculated using the Z-transformed dataset. The covariance matrix gives the relationship of each pixel intensity value of all images, each row being an eigenvector. The eigenvalues are calculated using the covariance matrix. Each eigenvalue corresponds to an eigenvector and represents the explained variance of this vector. The larger the eigenvalue, the more variance is captured along that eigenvector or principal component. <br>\n",
    "<span style=\"color:red\">\n",
    "hier Gleichung zur Berechnung von Covarianzmatrix & Berechnung für Eigenvalues einfügen? würde ich nicht, bissl unnötig <br>\n",
    "</span>\n",
    "The final step in a PCA is to choose the percentage of variance the data is supposed to describe and then removing all redundant Principal Components.\n",
    "We implemented PCA using NumPy by computing a covariance matrix using the Z-transformed data and then calculating the eigenvalues. <br>\n",
    "These were then resorted in ascending order by absolute value, since eigenvalues can have positive or negative values, where only the absolute holds the information of amount of variance explained.\n",
    "<span style=\"color:lightblue\">\n",
    "genaue Funktionsweise unserer PCA ist mMn nicht so wichtig, könnten wir kürzen\n",
    "</span>\n",
    "### K-nearest neighbors (KNN)\n",
    "K-nearest neighbors (KNN) is a non-parametric, supervised learning algorithm. In the context of our project it was used for classification but it can also be used for regression tasks. The KNN makes predictions based on the similarity of input data points to their neighboring data points by measuring the distance to all reference points and then finding the k nearest points. Euclidean distance was used as a distance calculation method. The class of the test data point is assigned by a so-called majority vote, meaning the class the majority of the nearest points have is selected. \n",
    "\n",
    "KNN belongs to the family of lazy-learning models. The algorithm stores the entire training dataset and uses it as reference for each testing data point instead of undergoing a training phase. As a consequence the run time is long, compared to algorithms that feature a training phase.\n",
    "To calculate the k nearest neighbors the size of k has to be determined. To find the optimal k we used the proof by exhaustion method. We let the KNN run with different numbers for k \n",
    "<span style=\"color:red\"> \n",
    "and then measured the performance and chose the optimal value for k with the highest accuracy. Normally this method is done using a validation data set, but we directly used the test data set. <br>\n",
    "</span>\n",
    "\n",
    "The aim for our KNN was to correctly classify images of clothing items, by comparing each test image with all images from the training data set and then comparing the predicted labels to the actual classification. The closest neighbors of the sample image were determined based on the smallest difference in intensity values between the images.\n",
    "<span style=\"color:red\"> \n",
    "In our code we defined the function \"dist\" to .....\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\"> \n",
    "- Vllt proof by exhaustion noch genauer erklären <br>\n",
    "- Einzelnen Steps unserer gecodeten KNN erklären\n",
    "</span>\n",
    "\n",
    "### KD-trees\n",
    "Since the KNN algorithm is not very time efficient due to it's way of operating, k-dimensioan trees or KD-trees are a tool to use clever data structures to optimize the classification process. K is defined by the number of properties each data point has, in this context it is the dimensions, so it corresponds to the number of PCs selected.\n",
    "The concept is based on the repeated division of the space along the median value of one of the dimensions. This can be shown in a binary tree structure. Each node is a splitting hyperplane, which is defined by one of the axis. \n",
    "<span style=\"color:red\"> \n",
    "The points are divided in the tree whether the coordinate value for that axis is bigger or smaller.\n",
    "</span>\n",
    "In which order the axes are chosen during division can vary, but the most common approach is to rotate through each of the dimensions.\n",
    "How many partitions of the space are made can be chosen by changing the leaf size. The leaf size is the maximal number of data points that can be contained in a final node at the bottom of the KD-tree. A smaller leaf size generates a larger tree with more partitions, making the construction time longer, a larger leaf size can result in an unbalanced tree and slow down the search time. However the leaf size does not affect the result of the query.\n",
    "<span style=\"color:red\"> \n",
    "- hier KD Tree einfügen\n",
    "</span>\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> \n",
    " - Pathfinding along tree to locate data point, go up on tree to locate any data points <br>\n",
    " -'hiding' in neighboring spaces <br>\n",
    " - Working uo the tree until the distance between the division axes exceeds distance of data point w/ smallest distance <br>\n",
    " </span>\n",
    "\n",
    "### Convolutional Neural Network (CNN)\n",
    " \n",
    "A convolutional neural network is a deep learning model based on the concept of an artificial neural network. It is mainly used for the analysis of visual data like images or videos, because it works by detecting patterns in an hierarchical strucutre. A CNN consists of different types of layers: convolutional layer, activation layer, pooling layer, fully connected layer, dropout layer and batch normalization layer. Not all these layers will be explained here. The most important are the convolutional layers. Each layer can consist of multiple convolutional filters. A filter is a small matrix with a defined number of rows and columns. It convolves over all blocks of pixels of the input image. The product of the filter is passed on to the next layer <span style=\"color:red\"> (or filter??) </span>. The filters are what detects the patterns. These become more and more sophisticated the deeper the network becomes. For example the filters in the first convolutional layer may only detect edges, while a filter in a deeper layer  recognizes complex shapes. <br>\n",
    "The convolutional layers set a CNN apart from other neural networks. Perks of a CNN are that all layers are fully connected, making it extremly complex when using images with many pixels. Also the features of an image are detected in the context of the surrounding pixels and not by their absolute intensity values, meaning edges are recognized as edges regardless of their brightness. Another advantage is that convolutional filters are trainable to be able to recognize different features and patterns.\n",
    "After each convolution layer a nonlinear activation function is applied. It determines which neurons are activated in the CNN. \n",
    "<span style=\"color:red\"> \n",
    "This way complex relationships betwen the input data and desired output can be modeled. \n",
    "</span>\n",
    "A standard activation function in CNN is ReLU, standing for Rectified Linear Unit. This function outputs the input directly if it is positive, and zero otherwise. <br>\n",
    "The pooling layers reduce the spatial dimensions of the input image to reduce the number of parameters and the computational load. It condenses the features in a region and summarizes it, which makes it more robust to variations of feature-position within the images. Most commonly used is the max pooling. The output layer then features only the maximum elements of all regions, translating to the most prominent features.\n",
    "<span style=\"color:lightblue\">\n",
    "\"These become more and more sophisticated the deeper the network becomes\" Das ist der Grundgedanke, aber wir wissen nicht, ob das wirklich so ist, deshalb würde ich eher schreiben, dass wir hoffen, dass es so funktioniert.\n",
    "\"After each convolution layer a nonlinear activation function is applied. It determines which neurons are activated in the CNN.\" Was ist damit gemeint? ReLU ist ja zumindest teilweise linear?\n",
    "\"This function outputs the input directly if it is positive, and zero otherwise.\" Nein, die gibt den Input einfach weiter, wenn er positiv ist\n",
    "\"Most commonly used is the max pooling\" Vorsicht mit solchen Aussagen, immer lieber abschwächen und sagen, dass es commonly used ist und dass wir es verwendet haben, dann sind wir fein raus\n",
    "\n",
    "### Evaluation methods\n",
    "\n",
    "To measure and compare the performance and effectiveness of the classification algorithms, the accuracy and confusion matrix were chosen as evaluation methods. \n",
    "The accuracy gives the percentage how many images were classified correctly. This is done by dividing the number of correct predictions by the number of total predictions. This evaluation method gives a good overview how well the algorithm is working, \n",
    "<span style=\"color:lightblue\">\n",
    "Was Accuracy ist müssen wir nicht wirklich erklären, das versteht ja jeder.\n",
    "<span style=\"color:red\"> \n",
    "but to get a deeper insight on common misclassifications, an enhanced version of a confusion matrix was used. A confusion matrix compares the predicted class of the objects to the actual class. In the cells the number of objects that fall into that category are given. To make our KNN & CNN confusion matrix more comparable, we used a chi-square test to see what cells have a higher or lower value than in a normal error distribution. This allows direct identification of which misclassifications were common and which algorithm has problems correctly predicting which class.\n",
    "</span>\n",
    "<span style=\"color:light\">\n",
    "chi-square würde ich als Begriff nicht verwenden, ich würde lieber sagen:\n",
    "\"We substracted the expected values of the confusion matrix from the observed values. Expected values means the values th would be observed if the algorithm was unbiased and each class could be detected just as good as every other class.\"\n",
    "Gerne auch mit alternativer Formulierung, aber so in der Art"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "### Optimal number of k and PCs\n",
    "After successful data preparation and implementation of KNN, the optimal number of principal component variance and number of neighbors was determined by running KNN for ks between 2 and 15 for the principal component variances 0.25, 0.5, 0.75, 0.8, 0.85, 0.9, 0.95 and 1. \n",
    "\n",
    "<div>\n",
    "<img src=\"../Plots/projectsummary.png\" width=600>\n",
    "</div>\n",
    "The colors indicate the principal component variance. The x-axis indicates the number of k, the y-axis the accuracy. \n",
    "The accuracy, that was achieved with the CNN is indicated as a line.\n",
    "\n",
    "This indicates, that the optimal combination of number of ks and principal component variance is k = 4 and var = 0.95.\n",
    "\n",
    "### KNN vs CNN: Confusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
